{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All yoga channels URLs to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = '/Applications/chromedriver'\n",
    "os.environ['webdriver.chrome.driver'] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/results?search_query=yoga&sp=EgIQAg%253D%253D'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'yoga'\n",
    "youtube_search = 'https://www.youtube.com/results?search_query='\n",
    "channel_filter = '&sp=EgIQAg%253D%253D'\n",
    "youtube_query = youtube_search + query.replace(' ', '+') + channel_filter\n",
    "youtube_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(youtube_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll until len(yoga_channel) is max\n",
    "for i in range(40):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "contents_div = soup.find('div', id='contents')\n",
    "\n",
    "yoga_channel = []\n",
    "for title in contents_div.find_all('a', id='main-link', attrs={'class': 'channel-link yt-simple-endpoint style-scope ytd-channel-renderer'}):\n",
    "    channel_url = f\"https://www.youtube.com{title['href']}\"\n",
    "    yoga_channel.append(channel_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yoga_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Attempt2/yoga_channel_607_att2.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(yoga_channel, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping \"About\" page of yoga channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_info_about(url):\n",
    "    \n",
    "    '''\n",
    "    Scrape data from 'About' section of YouTube channel.\n",
    "    ---\n",
    "    Input: url (string)\n",
    "    Output: dictionary\n",
    "    \n",
    "    '''\n",
    "    channel_data = {}\n",
    "    try:\n",
    "        response = requests.get((url + '/about'))\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"html5lib\")\n",
    "        contents_div = soup.find('div', id='content')\n",
    "    except:\n",
    "        channel_data = {} \n",
    "    try:\n",
    "        channel_data['name'] = contents_div.find('span', attrs={'class': 'about-channel-link-text'}).text.strip()\n",
    "    except:\n",
    "        channel_data['name'] = ''\n",
    "    try:\n",
    "        channel_data['total_views'] = contents_div.find('span', attrs={'class': 'about-stat'}).find('b').text.strip()\n",
    "    except:\n",
    "        channel_data['total_views'] = '' \n",
    "    try:\n",
    "        channel_data['joined_date'] = contents_div.find_all('span', attrs={'class': 'about-stat'})[1].text.strip()\n",
    "    except:\n",
    "        channel_data['joined_date'] = ''   \n",
    "    try:\n",
    "        channel_data['subscriber_count'] = contents_div.find('span', attrs={'class': 'yt-subscription-button-subscriber-count-branded-horizontal subscribed yt-uix-tooltip'}).text.strip()\n",
    "    except:\n",
    "        channel_data['subscriber_count'] = ''   \n",
    "    try:\n",
    "        channel_data['n_featured_channels'] = len(contents_div.find_all('h3', attrs={'class': 'yt-lockup-title'}))  #featured channels\n",
    "    except:\n",
    "        channel_data['n_featured_channels'] = ''\n",
    "    try:\n",
    "        channel_data['n_links'] = len(contents_div.find_all('a', attrs={'class': 'about-channel-link'}))  #marketing/social media links \n",
    "    except:   \n",
    "        channel_data['n_links'] = ''\n",
    "    try:\n",
    "        channel_data['description_length'] = len(contents_div.find('div', attrs={'class': 'about-description branded-page-box-padding'}).text.strip().replace('\\n', '')) #description length\n",
    "    except:   \n",
    "        channel_data['description_length'] = ''\n",
    "    try:\n",
    "        channel_data['description'] = contents_div.find('div', attrs={'class': 'about-description branded-page-box-padding'}).text.strip().replace('\\n', '')\n",
    "    except:   \n",
    "        channel_data['description'] = ''\n",
    "\n",
    "    return channel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yoga_channel is a list of yoga channel URLs\n",
    "channels_data = defaultdict()\n",
    "\n",
    "for channel in yoga_channel:\n",
    "    try:\n",
    "        channel_data = get_channel_info_about(channel)\n",
    "        channels_data[channel] = channel_data\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    time.sleep(.5+2*random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(channels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('channels_about_607_att2.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(channels_data, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping \"Videos\" page of yoga channels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First n videos (~30 per page), without scrolling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_page_info(url):\n",
    "    \n",
    "    try:\n",
    "        response = requests.get((url + '/videos'))\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"html5lib\")\n",
    "        contents_div = soup.find('div', id='content')\n",
    "    except:\n",
    "        videos_info = {}\n",
    "    \n",
    "    try:\n",
    "        titles = contents_div.find_all('a', attrs={'class': 'yt-uix-sessionlink yt-uix-tile-link spf-link yt-ui-ellipsis yt-ui-ellipsis-2'})\n",
    "    except:\n",
    "        videos_info = {}\n",
    "    \n",
    "    try:\n",
    "        views_days = contents_div.find_all('ul', attrs={'class': 'yt-lockup-meta-info'})\n",
    "    except:\n",
    "        views_days = ''\n",
    "    try:\n",
    "        duration = contents_div.find_all('span', attrs={'class': 'accessible-description'})\n",
    "    except:\n",
    "        duration = ''\n",
    "\n",
    "    videos_info = {}\n",
    "    count = 1 \n",
    "    for video in range(len(titles)):\n",
    "        video_info = {}\n",
    "        title = titles[video].get('title')\n",
    "        views_age = views_days[video].text.strip()\n",
    "        length = duration[video].text.strip()\n",
    "    \n",
    "        video_info['title'] = title\n",
    "        video_info['views_days'] = views_age\n",
    "        video_info['duration'] = length\n",
    "        videos_info[count] = video_info\n",
    "        count += 1\n",
    "    \n",
    "    return videos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yoga_channel is a list of yoga channel URLs\n",
    "videos_page_data = defaultdict()\n",
    "\n",
    "for channel in yoga_channel:\n",
    "    try:\n",
    "        channel_data = get_videos_page_info(channel)\n",
    "        videos_page_data[channel] = channel_data\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    time.sleep(.5+2*random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(videos_page_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Attempt2/channels_videos_page_590_att2.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(videos_page_data, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting links to videos by channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_links_scroll(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(chromedriver)\n",
    "        driver.get((url + '/videos'))\n",
    "        for i in range(45):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        res = soup.find_all('a', attrs={'class': 'yt-simple-endpoint style-scope ytd-grid-video-renderer'}) #yt-uix-sessionlink \n",
    "        videos = []\n",
    "        for i in res:\n",
    "            videos.append(i.get('href').replace('/watch?v=', ''))\n",
    "    except:\n",
    "        videos = []\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yoga_channel is a list of yoga channel URLs\n",
    "videos_links_by_channel = defaultdict()\n",
    "\n",
    "for channel in yoga_channel:\n",
    "    try:\n",
    "        channel_data = get_video_links_scroll(channel)\n",
    "        videos_links_by_channel[channel] = channel_data\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    time.sleep(.5+2*random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Attempt2/videos_links_by_channel_att2.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(videos_links_by_channel, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting datapoints by video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_info(url):\n",
    "    \n",
    "    video_data = {}\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"html5lib\")\n",
    "    except:\n",
    "        video_data = {}\n",
    "    \n",
    "    try:\n",
    "        video_data['Title'] = soup.find('span', attrs={'class': 'watch-title'}).text.strip()\n",
    "    except:\n",
    "        video_data['Title'] = ''\n",
    "    try:\n",
    "        video_data['Views'] = int(soup.find('div', attrs={'class': 'watch-view-count'}).text[:-6].replace(',', ''))\n",
    "    except:\n",
    "        video_data['Views'] = ''\n",
    "    try:\n",
    "        video_data['Description'] = soup.find('p', attrs={'id': 'eow-description'}).text\n",
    "    except:\n",
    "        video_data['Description'] = ''\n",
    "    try:\n",
    "        video_data['Publication Date'] = soup.find('strong', attrs={'class': 'watch-time-text'}).text.strip()\n",
    "    except:\n",
    "        video_data['Publication Date'] = ''\n",
    "    try:\n",
    "        video_data['Likes'] = int(soup.find('button', attrs={'title': 'I like this'}).text.replace(',', ''))\n",
    "    except:\n",
    "        video_data['Likes'] = ''\n",
    "    try:\n",
    "        video_data['Dislikes'] = int(soup.find('button', attrs={'title': 'I dislike this'}).text.replace(',', ''))\n",
    "    except:\n",
    "        video_data['Dislikes'] = ''\n",
    "    \n",
    "    return video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'links_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-4adbc06c7173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvideos_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinks_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://www.youtube.com/watch?v='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'links_dict' is not defined"
     ]
    }
   ],
   "source": [
    "videos_info = defaultdict()\n",
    "for key, val in links_dict.items():\n",
    "    try:\n",
    "        for i in val:\n",
    "            url = 'https://www.youtube.com/watch?v=' + i\n",
    "            videos_info[(key, i)] = get_video_info(url)\n",
    "    except:\n",
    "        continue \n",
    "        \n",
    "    time.sleep(.5+2*random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Attempt2/videos_info_att2.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(videos_info, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping comments with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
